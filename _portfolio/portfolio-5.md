---
title: "Efficient Attention Calculation (2024)"
collection: portfolio
year: 2024
order: 2
excerpt: "This project explores efficient transformer attention. I implemented a PQ‑tree‑based token subsampler to reduce complexity from O(L<sup>2</sup>) to O(L log L), and reproduced some of previously proposed attention approximation/subsampling methods.<br/><img class='portfolio-thumb' src='/images/attn.png' alt='Efficient attention (PQ-tree guided) diagram'>"
---

This project explores efficient transformer attention:

- Reproduced previously proposed attention approximation and subsampling mechanisms.
- Implemented a PQ‑tree–based token subsampler to reduce complexity from O(L<sup>2</sup>) to O(L log L).


